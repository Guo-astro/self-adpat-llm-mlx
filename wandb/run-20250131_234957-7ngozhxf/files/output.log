Model type: <class 'src.transformer_svd.TransformerLM'>
Is model a subclass of nn.Module? yes
Does model have a __call__ method? True
Model's parameters:
 - embedding.weight: (10000, 1024)
 - transformer.layers.0.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.0.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.0.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.0.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.0.ln1.weight: (1024,)
 - transformer.layers.0.ln1.bias: (1024,)
 - transformer.layers.0.ln2.weight: (1024,)
 - transformer.layers.0.ln2.bias: (1024,)
 - transformer.layers.0.linear1.weight: (4096, 1024)
 - transformer.layers.0.linear1.bias: (4096,)
 - transformer.layers.0.linear2.weight: (1024, 4096)
 - transformer.layers.0.linear2.bias: (1024,)
 - transformer.layers.1.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.1.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.1.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.1.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.1.ln1.weight: (1024,)
 - transformer.layers.1.ln1.bias: (1024,)
 - transformer.layers.1.ln2.weight: (1024,)
 - transformer.layers.1.ln2.bias: (1024,)
 - transformer.layers.1.linear1.weight: (4096, 1024)
 - transformer.layers.1.linear1.bias: (4096,)
 - transformer.layers.1.linear2.weight: (1024, 4096)
 - transformer.layers.1.linear2.bias: (1024,)
 - transformer.layers.2.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.2.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.2.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.2.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.2.ln1.weight: (1024,)
 - transformer.layers.2.ln1.bias: (1024,)
 - transformer.layers.2.ln2.weight: (1024,)
 - transformer.layers.2.ln2.bias: (1024,)
 - transformer.layers.2.linear1.weight: (4096, 1024)
 - transformer.layers.2.linear1.bias: (4096,)
 - transformer.layers.2.linear2.weight: (1024, 4096)
 - transformer.layers.2.linear2.bias: (1024,)
 - transformer.layers.3.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.3.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.3.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.3.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.3.ln1.weight: (1024,)
 - transformer.layers.3.ln1.bias: (1024,)
 - transformer.layers.3.ln2.weight: (1024,)
 - transformer.layers.3.ln2.bias: (1024,)
 - transformer.layers.3.linear1.weight: (4096, 1024)
 - transformer.layers.3.linear1.bias: (4096,)
 - transformer.layers.3.linear2.weight: (1024, 4096)
 - transformer.layers.3.linear2.bias: (1024,)
 - transformer.layers.4.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.4.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.4.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.4.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.4.ln1.weight: (1024,)
 - transformer.layers.4.ln1.bias: (1024,)
 - transformer.layers.4.ln2.weight: (1024,)
 - transformer.layers.4.ln2.bias: (1024,)
 - transformer.layers.4.linear1.weight: (4096, 1024)
 - transformer.layers.4.linear1.bias: (4096,)
 - transformer.layers.4.linear2.weight: (1024, 4096)
 - transformer.layers.4.linear2.bias: (1024,)
 - transformer.layers.5.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.5.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.5.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.5.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.5.ln1.weight: (1024,)
 - transformer.layers.5.ln1.bias: (1024,)
 - transformer.layers.5.ln2.weight: (1024,)
 - transformer.layers.5.ln2.bias: (1024,)
 - transformer.layers.5.linear1.weight: (4096, 1024)
 - transformer.layers.5.linear1.bias: (4096,)
 - transformer.layers.5.linear2.weight: (1024, 4096)
 - transformer.layers.5.linear2.bias: (1024,)
 - transformer.layers.6.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.6.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.6.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.6.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.6.ln1.weight: (1024,)
 - transformer.layers.6.ln1.bias: (1024,)
 - transformer.layers.6.ln2.weight: (1024,)
 - transformer.layers.6.ln2.bias: (1024,)
 - transformer.layers.6.linear1.weight: (4096, 1024)
 - transformer.layers.6.linear1.bias: (4096,)
 - transformer.layers.6.linear2.weight: (1024, 4096)
 - transformer.layers.6.linear2.bias: (1024,)
 - transformer.layers.7.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.7.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.7.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.7.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.7.ln1.weight: (1024,)
 - transformer.layers.7.ln1.bias: (1024,)
 - transformer.layers.7.ln2.weight: (1024,)
 - transformer.layers.7.ln2.bias: (1024,)
 - transformer.layers.7.linear1.weight: (4096, 1024)
 - transformer.layers.7.linear1.bias: (4096,)
 - transformer.layers.7.linear2.weight: (1024, 4096)
 - transformer.layers.7.linear2.bias: (1024,)
 - transformer.layers.8.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.8.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.8.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.8.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.8.ln1.weight: (1024,)
 - transformer.layers.8.ln1.bias: (1024,)
 - transformer.layers.8.ln2.weight: (1024,)
 - transformer.layers.8.ln2.bias: (1024,)
 - transformer.layers.8.linear1.weight: (4096, 1024)
 - transformer.layers.8.linear1.bias: (4096,)
 - transformer.layers.8.linear2.weight: (1024, 4096)
 - transformer.layers.8.linear2.bias: (1024,)
 - transformer.layers.9.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.9.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.9.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.9.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.9.ln1.weight: (1024,)
 - transformer.layers.9.ln1.bias: (1024,)
 - transformer.layers.9.ln2.weight: (1024,)
 - transformer.layers.9.ln2.bias: (1024,)
 - transformer.layers.9.linear1.weight: (4096, 1024)
 - transformer.layers.9.linear1.bias: (4096,)
 - transformer.layers.9.linear2.weight: (1024, 4096)
 - transformer.layers.9.linear2.bias: (1024,)
 - transformer.layers.10.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.10.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.10.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.10.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.10.ln1.weight: (1024,)
 - transformer.layers.10.ln1.bias: (1024,)
 - transformer.layers.10.ln2.weight: (1024,)
 - transformer.layers.10.ln2.bias: (1024,)
 - transformer.layers.10.linear1.weight: (4096, 1024)
 - transformer.layers.10.linear1.bias: (4096,)
 - transformer.layers.10.linear2.weight: (1024, 4096)
 - transformer.layers.10.linear2.bias: (1024,)
 - transformer.layers.11.attention.query_proj.weight: (1024, 1024)
 - transformer.layers.11.attention.key_proj.weight: (1024, 1024)
 - transformer.layers.11.attention.value_proj.weight: (1024, 1024)
 - transformer.layers.11.attention.out_proj.weight: (1024, 1024)
 - transformer.layers.11.ln1.weight: (1024,)
 - transformer.layers.11.ln1.bias: (1024,)
 - transformer.layers.11.ln2.weight: (1024,)
 - transformer.layers.11.ln2.bias: (1024,)
 - transformer.layers.11.linear1.weight: (4096, 1024)
 - transformer.layers.11.linear1.bias: (4096,)
 - transformer.layers.11.linear2.weight: (1024, 4096)
 - transformer.layers.11.linear2.bias: (1024,)
 - transformer.ln.weight: (1024,)
 - transformer.ln.bias: (1024,)
 - out_proj.weight: (10000, 1024)
 - out_proj.bias: (10000,)
Total Parameters (excluding 'embedding' and 'pe'): 161357584
╭─────────────── Model Information ────────────────╮
│ Training a Transformer with 161.358 M Parameters │
╰──────────────────────────────────────────────────╯
Model is callable. Output shape: (1, 1024, 10000)
Traceback (most recent call last):
  File "/Users/guo/OSS/self-adpat-llm-mlx/main.py", line 563, in <module>
    main(args)
  File "/Users/guo/OSS/self-adpat-llm-mlx/main.py", line 263, in main
    loss = step(inputs, targets)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/guo/OSS/self-adpat-llm-mlx/main.py", line 229, in step
    loss = loss_fn(model, inputs, targets, reduce=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/guo/OSS/self-adpat-llm-mlx/main.py", line 179, in loss_fn
    return float(losses.mean())
           ^^^^^^^^^^^^^^^^^^^^
ValueError: [eval] Attempting to eval an array during function transformations like compile or vmap is not allowed.
